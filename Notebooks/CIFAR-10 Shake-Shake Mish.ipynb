{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\n\nimport time\n\n# import pytorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import SGD,Adam,lr_scheduler\nfrom torch.utils.data import random_split\nimport torchvision\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define transformations for train\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=.40),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n\n# define transformations for test\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n\n# define training dataloader\ndef get_training_dataloader(train_transform, batch_size=128, num_workers=0, shuffle=True):\n    \"\"\" return training dataloader\n    Args:\n        train_transform: transfroms for train dataset\n        path: path to cifar100 training python dataset\n        batch_size: dataloader batchsize\n        num_workers: dataloader num_works\n        shuffle: whether to shuffle \n    Returns: train_data_loader:torch dataloader object\n    \"\"\"\n\n    transform_train = train_transform\n    cifar10_training = torchvision.datasets.CIFAR10(root='.', train=True, download=True, transform=transform_train)\n    cifar10_training_loader = DataLoader(\n        cifar10_training, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n\n    return cifar10_training_loader\n\n# define test dataloader\ndef get_testing_dataloader(test_transform, batch_size=128, num_workers=0, shuffle=True):\n    \"\"\" return training dataloader\n    Args:\n        test_transform: transforms for test dataset\n        path: path to cifar100 test python dataset\n        batch_size: dataloader batchsize\n        num_workers: dataloader num_works\n        shuffle: whether to shuffle \n    Returns: cifar100_test_loader:torch dataloader object\n    \"\"\"\n\n    transform_test = test_transform\n    cifar10_test = torchvision.datasets.CIFAR10(root='.', train=False, download=True, transform=transform_test)\n    cifar10_test_loader = DataLoader(\n        cifar10_test, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n\n    return cifar10_test_loader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# implement mish activation function\ndef f_mish(input):\n    '''\n    Applies the mish function element-wise:\n    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n    '''\n    return input * torch.tanh(F.softplus(input))\n\n# implement class wrapper for mish activation function\nclass mish(nn.Module):\n    '''\n    Applies the mish function element-wise:\n    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n\n    Shape:\n        - Input: (N, *) where * means, any number of additional\n          dimensions\n        - Output: (N, *), same shape as the input\n\n    Examples:\n        >>> m = mish()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n\n    '''\n    def __init__(self):\n        '''\n        Init method.\n        '''\n        super().__init__()\n\n    def forward(self, input):\n        '''\n        Forward pass of the function.\n        '''\n        return f_mish(input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# implement swish activation function\ndef f_swish(input):\n    '''\n    Applies the swish function element-wise:\n    swish(x) = x * sigmoid(x)\n    '''\n    return input * torch.sigmoid(input)\n\n# implement class wrapper for swish activation function\nclass swish(nn.Module):\n    '''\n    Applies the swish function element-wise:\n    swish(x) = x * sigmoid(x)\n\n    Shape:\n        - Input: (N, *) where * means, any number of additional\n          dimensions\n        - Output: (N, *), same shape as the input\n\n    Examples:\n        >>> m = swish()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n\n    '''\n    def __init__(self):\n        '''\n        Init method.\n        '''\n        super().__init__()\n\n    def forward(self, input):\n        '''\n        Forward pass of the function.\n        '''\n        return f_swish(input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# see https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html\n# see https://discuss.pytorch.org/t/why-input-is-tensor-in-the-forward-function-when-extending-torch-autograd/9039\nclass ShakeShake(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input1, input2, alpha, beta=None):\n        ctx.save_for_backward(input1, input2, alpha, beta)\n        out = alpha * input1 + (1 - alpha) * input2\n        return out\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input1, input2, alpha, beta = ctx.saved_tensors\n        grad_input1 = beta * grad_output\n        grad_input2 = (1 - beta) * grad_output\n        return grad_input1, grad_input2, None, None\n\n\nclass SkippingBranch(nn.Module):\n    def __init__(self, inplanes, stride=2):\n        super(SkippingBranch, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, inplanes, kernel_size=1, stride=1, \n                              padding=0, bias=False)\n        self.conv2 = nn.Conv2d(inplanes, inplanes, kernel_size=1, stride=1, \n                              padding=0, bias=False)\n        self.avg_pool = nn.AvgPool2d(kernel_size=1, stride=stride, padding=0)    \n\n    def forward(self, x):\n        out1 = self.conv1(self.avg_pool(x))\n        shift_x = x[:, :, 1:, 1:]\n        shift_x= F.pad(shift_x, (0, 1, 0, 1))\n        out2 = self.conv2(self.avg_pool(shift_x))\n        out = torch.cat([out1, out2], dim=1)\n        return out\n\n\nclass ResidualBranch(nn.Module):\n    def __init__(self, inplanes, planes, stride=1):\n        super(ResidualBranch, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, \n                               stride=stride, padding=1, bias=False) \n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, \n                               stride=1, padding=1, bias=False) \n        self.bn2 = nn.BatchNorm2d(planes)\n\n    def forward(self, x):\n        out = self.bn1(self.conv1(F.relu(x, inplace=False)))\n        out = self.bn2(self.conv2(F.relu(out, inplace=False)))\n        return out\n\n\nclass ShakeBlock(nn.Module):\n    def __init__(self, inplanes, planes, stride=1):\n        super(ShakeBlock, self).__init__()\n        self.residual_branch1 = ResidualBranch(inplanes, planes, stride)\n        self.residual_branch2 = ResidualBranch(inplanes, planes, stride)\n\n        if inplanes != planes:\n            self.skipping_branch = SkippingBranch(inplanes, stride)\n        else:\n            self.skipping_branch = nn.Sequential()\n\n        self.shake_shake = ShakeShake.apply\n\n    def forward(self, x):\n        residual = x\n        out1 = self.residual_branch1(x)\n        out2 = self.residual_branch2(x)\n        \n        batch_size = out1.size(0)\n        if self.training:        \n            alpha = torch.rand(batch_size).to(device)\n            beta = torch.rand(batch_size).to(device)\n            beta = beta.view(batch_size, 1, 1, 1)\n            alpha = alpha.view(batch_size, 1, 1, 1)\n            out = self.shake_shake(out1, out2, alpha, beta)\n        else:\n            alpha = torch.Tensor([0.5]).to(device)\n            out = self.shake_shake(out1, out2, alpha)\n\n        skip = self.skipping_branch(residual)\n        return out + skip\n\n\nclass ShakeResNet(nn.Module):\n    def __init__(self, block, num_classes=10):\n        super(ShakeResNet, self).__init__()\n        self.inplanes = 16\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, \n                               padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.relu = mish() # changed to mish here\n\n        self.stage1 = self._make_stage(block, 32, 4, stride=1)\n        self.stage2 = self._make_stage(block, 64, 4, stride=2) \n        self.stage3 = self._make_stage(block, 128, 4, stride=2)  \n        self.avg_pool = nn.AvgPool2d(8, stride=1)\n        self.fc_out = nn.Linear(128, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', \n                                        nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_stage(self, block, planes, blocks, stride=1):\n        layers = []\n        layers.append(block(self.inplanes, planes, stride))\n        self.inplanes = planes\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.relu(self.bn1(self.conv1(x))) # is already initialized with swish above\n\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n\n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc_out(x)\n        return x\n\n\ndef shake_shake(**kwargs):\n    model = ShakeResNet(ShakeBlock, **kwargs) \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainloader = get_training_dataloader(train_transform)\ntestloader = get_testing_dataloader(test_transform)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 1\nbatch_size = 128\nlearning_rate = 0.001\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = shake_shake() # Shake-shake ReLU","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set loss function\ncriterion = nn.CrossEntropyLoss()\n\n# set optimizer, only train the classifier parameters, feature parameters are frozen\noptimizer = Adam(model.parameters(), lr=learning_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_stats = pd.DataFrame(columns = ['Epoch', 'Time per epoch', 'Avg time per step', 'Train loss', 'Train accuracy', 'Train top-3 accuracy','Test loss', 'Test accuracy', 'Test top-3 accuracy']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train the model\nmodel.to(device)\n\nsteps = 0\nrunning_loss = 0\nfor epoch in range(epochs):\n    \n    since = time.time()\n    \n    train_accuracy = 0\n    top3_train_accuracy = 0 \n    for inputs, labels in trainloader:\n        steps += 1\n        # Move input and label tensors to the default device\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        logps = model.forward(inputs)\n        loss = criterion(logps, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        \n        # calculate train top-1 accuracy\n        ps = torch.exp(logps)\n        top_p, top_class = ps.topk(1, dim=1)\n        equals = top_class == labels.view(*top_class.shape)\n        train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n        \n        # Calculate train top-3 accuracy\n        np_top3_class = ps.topk(3, dim=1)[1].cpu().numpy()\n        target_numpy = labels.cpu().numpy()\n        top3_train_accuracy += np.mean([1 if target_numpy[i] in np_top3_class[i] else 0 for i in range(0, len(target_numpy))])\n        \n    time_elapsed = time.time() - since\n    \n    test_loss = 0\n    test_accuracy = 0\n    top3_test_accuracy = 0\n    model.eval()\n    with torch.no_grad():\n        for inputs, labels in testloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            logps = model.forward(inputs)\n            batch_loss = criterion(logps, labels)\n\n            test_loss += batch_loss.item()\n\n            # Calculate test top-1 accuracy\n            ps = torch.exp(logps)\n            top_p, top_class = ps.topk(1, dim=1)\n            equals = top_class == labels.view(*top_class.shape)\n            test_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n            \n            # Calculate test top-3 accuracy\n            np_top3_class = ps.topk(3, dim=1)[1].cpu().numpy()\n            target_numpy = labels.cpu().numpy()\n            top3_test_accuracy += np.mean([1 if target_numpy[i] in np_top3_class[i] else 0 for i in range(0, len(target_numpy))])\n\n    print(f\"Epoch {epoch+1}/{epochs}.. \"\n          f\"Time per epoch: {time_elapsed:.4f}.. \"\n          f\"Average time per step: {time_elapsed/len(trainloader):.4f}.. \"\n          f\"Train loss: {running_loss/len(trainloader):.4f}.. \"\n          f\"Train accuracy: {train_accuracy/len(trainloader):.4f}.. \"\n          f\"Top-3 train accuracy: {top3_train_accuracy/len(trainloader):.4f}.. \"\n          f\"Test loss: {test_loss/len(testloader):.4f}.. \"\n          f\"Test accuracy: {test_accuracy/len(testloader):.4f}.. \"\n          f\"Top-3 test accuracy: {top3_test_accuracy/len(testloader):.4f}\")\n\n    train_stats = train_stats.append({'Epoch': epoch, 'Time per epoch':time_elapsed, 'Avg time per step': time_elapsed/len(trainloader), 'Train loss' : running_loss/len(trainloader), 'Train accuracy': train_accuracy/len(trainloader), 'Train top-3 accuracy':top3_train_accuracy/len(trainloader),'Test loss' : test_loss/len(testloader), 'Test accuracy': test_accuracy/len(testloader), 'Test top-3 accuracy':top3_test_accuracy/len(testloader)}, ignore_index=True)\n\n    running_loss = 0\n    model.train()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_stats.to_csv('train_log_Shake-Shake_Mish.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}